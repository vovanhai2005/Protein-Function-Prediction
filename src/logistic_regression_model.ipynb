{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# For GPU acceleration on Kaggle, CuML is pre-installed\n",
    "# On local machines: pip install cuml-cu11 cupy-cuda11x\n",
    "# !pip install pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04338c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Detection and Library Import\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.multiclass import OneVsRestClassifier as cuOneVsRestClassifier\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"✓ GPU libraries loaded successfully!\")\n",
    "    print(f\"GPU Device: {cp.cuda.Device()}\")\n",
    "    print(f\"GPU Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB total\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    print(\"⚠ GPU libraries not available, using CPU (sklearn)\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d316e69",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8baf1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for Kaggle environment\n",
    "DATA_DIR = \"/kaggle/input/cafa-5-protein-function-prediction\"\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# Model parameters\n",
    "TOP_N_TERMS = 600  # Number of most common GO terms to predict\n",
    "TEST_SIZE = 0.15  # Validation split ratio\n",
    "RANDOM_STATE = 42\n",
    "PREDICTION_THRESHOLD = 0.3  # Threshold for final predictions\n",
    "\n",
    "# Logistic Regression parameters\n",
    "LR_MAX_ITER = 1000\n",
    "LR_C = 0.5  # Regularization strength\n",
    "LR_SOLVER = 'lbfgs'\n",
    "\n",
    "print(f\"Configuration set!\")\n",
    "print(f\"Top GO terms to predict: {TOP_N_TERMS}\")\n",
    "print(f\"Prediction threshold: {PREDICTION_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec1930",
   "metadata": {},
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3aff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training terms\n",
    "print(\"Loading training data...\")\n",
    "train_terms = pd.read_csv(os.path.join(DATA_DIR, 'Train', 'train_terms.tsv'), sep='\\t')\n",
    "print(f\"Loaded {len(train_terms)} term annotations\")\n",
    "print(train_terms.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bac7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taxonomy (optional - can be used for additional features)\n",
    "train_taxonomy = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'Train', 'train_taxonomy.tsv'), \n",
    "    sep='\\t', \n",
    "    header=None, \n",
    "    names=['EntryID', 'TaxonID']\n",
    ")\n",
    "print(f\"Loaded {len(train_taxonomy)} taxonomy entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8fb94",
   "metadata": {},
   "source": [
    "## 3. Parse FASTA Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aecaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta(fasta_path):\n",
    "    \"\"\"\n",
    "    Parse FASTA file and return dict of protein_id -> sequence\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(fasta_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence\n",
    "                if current_id is not None:\n",
    "                    sequences[current_id] = ''.join(current_seq)\n",
    "                \n",
    "                # Extract ID from header (format: >sp|ID|... or >ID taxon)\n",
    "                match = re.search(r'\\|([A-Z0-9]+)\\|', line)\n",
    "                if match:\n",
    "                    current_id = match.group(1)\n",
    "                else:\n",
    "                    current_id = line[1:].split()[0]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        \n",
    "        # Don't forget the last sequence\n",
    "        if current_id is not None:\n",
    "            sequences[current_id] = ''.join(current_seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a326edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse training sequences\n",
    "print(\"Parsing training FASTA sequences...\")\n",
    "train_sequences = parse_fasta(os.path.join(DATA_DIR, 'Train', 'train_sequences.fasta'))\n",
    "print(f\"Parsed {len(train_sequences)} protein sequences\")\n",
    "\n",
    "# Show example\n",
    "sample_id = list(train_sequences.keys())[0]\n",
    "print(f\"\\nExample: {sample_id}\")\n",
    "print(f\"Sequence length: {len(train_sequences[sample_id])}\")\n",
    "print(f\"First 50 amino acids: {train_sequences[sample_id][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3aef20",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa671f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features from amino acid sequence:\n",
    "    - Amino acid composition (20 features)\n",
    "    - Sequence length and log-length\n",
    "    - Physicochemical properties\n",
    "    - Dipeptide frequencies (20 most common)\n",
    "    - N-terminal and C-terminal composition\n",
    "    \"\"\"\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    features = {}\n",
    "    \n",
    "    # Basic properties\n",
    "    seq_len = len(sequence)\n",
    "    features['length'] = seq_len\n",
    "    features['log_length'] = np.log1p(seq_len)\n",
    "    \n",
    "    if seq_len == 0:\n",
    "        # Return zero features for empty sequences\n",
    "        for aa in amino_acids:\n",
    "            features[f'aa_{aa}'] = 0.0\n",
    "        \n",
    "        # Physicochemical properties\n",
    "        for prop in ['hydrophobic', 'polar', 'charged', 'positive', 'negative', \n",
    "                     'aromatic', 'aliphatic', 'tiny', 'small', 'large']:\n",
    "            features[f'{prop}_ratio'] = 0.0\n",
    "        \n",
    "        # Dipeptides\n",
    "        common_dipeptides = ['AA', 'AL', 'LL', 'LA', 'GG', 'VA', 'AV', 'EE', 'KK', 'RR',\n",
    "                            'SS', 'TT', 'PP', 'DD', 'NN', 'QQ', 'FF', 'WW', 'YY', 'CC']\n",
    "        for dp in common_dipeptides:\n",
    "            features[f'di_{dp}'] = 0.0\n",
    "        \n",
    "        # Terminal composition\n",
    "        for aa in ['A', 'M', 'G', 'S', 'T', 'L', 'K', 'R']:\n",
    "            features[f'n_term_{aa}'] = 0.0\n",
    "            features[f'c_term_{aa}'] = 0.0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # Amino acid composition (normalized)\n",
    "    aa_counts = Counter(sequence)\n",
    "    for aa in amino_acids:\n",
    "        features[f'aa_{aa}'] = aa_counts.get(aa, 0) / seq_len\n",
    "    \n",
    "    # Physicochemical properties\n",
    "    hydrophobic = 'AVILMFWP'\n",
    "    polar = 'STNQCY'\n",
    "    positive = 'RK'\n",
    "    negative = 'DE'\n",
    "    aromatic = 'FWY'\n",
    "    aliphatic = 'AILV'\n",
    "    tiny = 'AGSCT'\n",
    "    small = 'ABCDGNPSTV'\n",
    "    large = 'EFHIKLMQRWY'\n",
    "    \n",
    "    features['hydrophobic_ratio'] = sum(aa_counts.get(aa, 0) for aa in hydrophobic) / seq_len\n",
    "    features['polar_ratio'] = sum(aa_counts.get(aa, 0) for aa in polar) / seq_len\n",
    "    features['charged_ratio'] = sum(aa_counts.get(aa, 0) for aa in positive + negative) / seq_len\n",
    "    features['positive_ratio'] = sum(aa_counts.get(aa, 0) for aa in positive) / seq_len\n",
    "    features['negative_ratio'] = sum(aa_counts.get(aa, 0) for aa in negative) / seq_len\n",
    "    features['aromatic_ratio'] = sum(aa_counts.get(aa, 0) for aa in aromatic) / seq_len\n",
    "    features['aliphatic_ratio'] = sum(aa_counts.get(aa, 0) for aa in aliphatic) / seq_len\n",
    "    features['tiny_ratio'] = sum(aa_counts.get(aa, 0) for aa in tiny) / seq_len\n",
    "    features['small_ratio'] = sum(aa_counts.get(aa, 0) for aa in small) / seq_len\n",
    "    features['large_ratio'] = sum(aa_counts.get(aa, 0) for aa in large) / seq_len\n",
    "    \n",
    "    # Dipeptide frequencies\n",
    "    dipeptides = [sequence[i:i+2] for i in range(len(sequence)-1)]\n",
    "    dipeptide_counts = Counter(dipeptides)\n",
    "    \n",
    "    common_dipeptides = ['AA', 'AL', 'LL', 'LA', 'GG', 'VA', 'AV', 'EE', 'KK', 'RR',\n",
    "                        'SS', 'TT', 'PP', 'DD', 'NN', 'QQ', 'FF', 'WW', 'YY', 'CC']\n",
    "    for dp in common_dipeptides:\n",
    "        features[f'di_{dp}'] = dipeptide_counts.get(dp, 0) / max(len(dipeptides), 1)\n",
    "    \n",
    "    # N-terminal and C-terminal composition\n",
    "    terminal_len = max(10, int(seq_len * 0.1))\n",
    "    n_terminal = sequence[:terminal_len]\n",
    "    c_terminal = sequence[-terminal_len:]\n",
    "    \n",
    "    n_term_counts = Counter(n_terminal)\n",
    "    c_term_counts = Counter(c_terminal)\n",
    "    \n",
    "    for aa in ['A', 'M', 'G', 'S', 'T', 'L', 'K', 'R']:\n",
    "        features[f'n_term_{aa}'] = n_term_counts.get(aa, 0) / len(n_terminal)\n",
    "        features[f'c_term_{aa}'] = c_term_counts.get(aa, 0) / len(c_terminal)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a67159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all sequences\n",
    "print(\"Extracting features from sequences...\")\n",
    "feature_data = []\n",
    "entry_ids = []\n",
    "\n",
    "for entry_id, sequence in tqdm(train_sequences.items(), desc=\"Extracting features\"):\n",
    "    features = extract_features(sequence)\n",
    "    feature_data.append(features)\n",
    "    entry_ids.append(entry_id)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_df = pd.DataFrame(feature_data, index=entry_ids)\n",
    "print(f\"\\nExtracted features shape: {X_df.shape}\")\n",
    "print(f\"Feature columns: {X_df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df.values)\n",
    "print(f\"Feature scaling complete. Shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e9764",
   "metadata": {},
   "source": [
    "## 5. Prepare Labels (GO Terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels - multi-label format\n",
    "print(\"Preparing GO term labels...\")\n",
    "entry_to_terms = defaultdict(set)\n",
    "\n",
    "for idx, row in tqdm(train_terms.iterrows(), total=len(train_terms), desc=\"Building label mapping\"):\n",
    "    entry_to_terms[row['EntryID']].add(row['term'])\n",
    "\n",
    "# Convert sets to lists\n",
    "entry_to_terms = {k: list(v) for k, v in entry_to_terms.items()}\n",
    "\n",
    "print(f\"Built mapping for {len(entry_to_terms)} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d841582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include entries with both sequences and GO terms\n",
    "valid_entry_ids = [eid for eid in entry_ids if eid in entry_to_terms]\n",
    "print(f\"Found {len(valid_entry_ids)} entries with both sequences and GO terms\")\n",
    "\n",
    "# Prepare final datasets\n",
    "X = X_scaled[X_df.index.isin(valid_entry_ids)]\n",
    "y_labels = [entry_to_terms[eid] for eid in valid_entry_ids]\n",
    "\n",
    "print(f\"\\nFinal training data shape: {X.shape}\")\n",
    "print(f\"Number of samples with labels: {len(y_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "term_counts = Counter([term for terms in y_labels for term in terms])\n",
    "print(f\"\\nTotal unique GO terms: {len(term_counts)}\")\n",
    "print(f\"\\nTop 20 most common GO terms:\")\n",
    "for term, count in term_counts.most_common(20):\n",
    "    print(f\"  {term}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c594077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to top N most common terms\n",
    "top_terms = [term for term, count in term_counts.most_common(TOP_N_TERMS)]\n",
    "print(f\"\\nFocusing on top {TOP_N_TERMS} most common GO terms\")\n",
    "\n",
    "# Filter labels to top terms\n",
    "filtered_y_labels = [[term for term in terms if term in top_terms] for terms in y_labels]\n",
    "\n",
    "# Binarize labels\n",
    "mlb_filtered = MultiLabelBinarizer(classes=top_terms)\n",
    "y_filtered = mlb_filtered.fit_transform(filtered_y_labels)\n",
    "\n",
    "print(f\"Filtered label matrix shape: {y_filtered.shape}\")\n",
    "print(f\"Average labels per protein: {y_filtered.sum() / len(y_filtered):.2f}\")\n",
    "print(f\"Sparsity: {1 - (y_filtered.sum() / y_filtered.size):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012579f",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e640058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_filtered, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining labels per sample: {y_train.sum(axis=1).mean():.2f}\")\n",
    "print(f\"Test labels per sample: {y_test.sum(axis=1).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d157d1",
   "metadata": {},
   "source": [
    "## 7. Train Multi-Label Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9883993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "print(\"Building multi-label logistic regression model...\")\n",
    "print(f\"Training {len(top_terms)} binary classifiers in parallel...\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"Using GPU acceleration with CuML!\")\n",
    "    model = cuOneVsRestClassifier(\n",
    "        cuLogisticRegression(\n",
    "            max_iter=LR_MAX_ITER,\n",
    "            C=LR_C,\n",
    "            penalty='l2',\n",
    "            verbose=0\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    print(\"Using CPU with scikit-learn\")\n",
    "    model = OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            max_iter=LR_MAX_ITER,\n",
    "            solver=LR_SOLVER,\n",
    "            C=LR_C,\n",
    "            penalty='l2',\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=1,\n",
    "            verbose=0\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "print(\"Model configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44af16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nTraining model... This may take several minutes...\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    # Convert to CuPy arrays for GPU training\n",
    "    X_train_gpu = cp.asarray(X_train)\n",
    "    y_train_gpu = cp.asarray(y_train)\n",
    "    model.fit(X_train_gpu, y_train_gpu)\n",
    "    # Free GPU memory\n",
    "    del X_train_gpu, y_train_gpu\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "else:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525319c9",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    X_test_gpu = cp.asarray(X_test)\n",
    "    y_pred = cp.asnumpy(model.predict(X_test_gpu))\n",
    "    y_pred_proba = cp.asnumpy(model.predict_proba(X_test_gpu))\n",
    "    del X_test_gpu\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "else:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Predictions complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e077dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "f1_samples = f1_score(y_test, y_pred, average='samples', zero_division=0)\n",
    "precision_samples = precision_score(y_test, y_pred, average='samples', zero_division=0)\n",
    "recall_samples = recall_score(y_test, y_pred, average='samples', zero_division=0)\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSample-based metrics:\")\n",
    "print(f\"  F1-Score:  {f1_samples:.4f}\")\n",
    "print(f\"  Precision: {precision_samples:.4f}\")\n",
    "print(f\"  Recall:    {recall_samples:.4f}\")\n",
    "\n",
    "print(f\"\\nMicro-averaged metrics:\")\n",
    "print(f\"  F1-Score:  {f1_micro:.4f}\")\n",
    "print(f\"  Precision: {precision_micro:.4f}\")\n",
    "print(f\"  Recall:    {recall_micro:.4f}\")\n",
    "\n",
    "print(f\"\\nOther metrics:\")\n",
    "print(f\"  F1-Macro:    {f1_macro:.4f}\")\n",
    "print(f\"  F1-Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"  Hamming Loss: {hamming:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad82a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "threshold_results = []\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    f1_thresh = f1_score(y_test, y_pred_thresh, average='samples', zero_division=0)\n",
    "    coverage = (y_pred_thresh.sum(axis=1) > 0).mean()\n",
    "    avg_preds = y_pred_thresh.sum(axis=1).mean()\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'f1': f1_thresh,\n",
    "        'coverage': coverage,\n",
    "        'avg_predictions': avg_preds\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold {threshold:.1f}: F1={f1_thresh:.4f}, Coverage={coverage:.2%}, Avg predictions={avg_preds:.2f}\")\n",
    "\n",
    "# Find best threshold\n",
    "best_result = max(threshold_results, key=lambda x: x['f1'])\n",
    "print(f\"\\n✓ Best threshold: {best_result['threshold']:.1f} (F1={best_result['f1']:.4f})\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f310a66",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for top GO terms\n",
    "print(\"\\nAnalyzing feature importance for top GO terms...\\n\")\n",
    "\n",
    "for i in range(min(5, len(top_terms))):\n",
    "    term = top_terms[i]\n",
    "    estimator = model.estimators_[i]\n",
    "    coef = estimator.coef_[0]\n",
    "    \n",
    "    # Get top positive and negative features\n",
    "    top_pos_idx = np.argsort(coef)[-5:][::-1]\n",
    "    top_neg_idx = np.argsort(coef)[:5]\n",
    "    \n",
    "    print(f\"GO Term: {term}\")\n",
    "    print(f\"  Top positive features:\")\n",
    "    for idx in top_pos_idx:\n",
    "        print(f\"    {X_df.columns[idx]}: {coef[idx]:.4f}\")\n",
    "    print(f\"  Top negative features:\")\n",
    "    for idx in top_neg_idx:\n",
    "        print(f\"    {X_df.columns[idx]}: {coef[idx]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011e451",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessing objects\n",
    "print(\"Saving model...\")\n",
    "\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'mlb': mlb_filtered,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': X_df.columns.tolist(),\n",
    "    'top_terms': top_terms,\n",
    "    'threshold': PREDICTION_THRESHOLD,\n",
    "    'performance': {\n",
    "        'f1_samples': f1_samples,\n",
    "        'precision_samples': precision_samples,\n",
    "        'recall_samples': recall_samples,\n",
    "        'f1_micro': f1_micro\n",
    "    }\n",
    "}\n",
    "\n",
    "model_path = os.path.join(OUTPUT_DIR, 'logistic_regression_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"✓ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1f902",
   "metadata": {},
   "source": [
    "## 11. Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486926bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse test sequences\n",
    "print(\"Loading test sequences...\")\n",
    "test_sequences = parse_fasta(os.path.join(DATA_DIR, 'Test', 'testsuperset.fasta'))\n",
    "print(f\"Loaded {len(test_sequences)} test sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for test sequences\n",
    "print(\"Extracting features from test sequences...\")\n",
    "test_feature_data = []\n",
    "test_entry_ids = []\n",
    "\n",
    "for entry_id, sequence in tqdm(test_sequences.items(), desc=\"Extracting test features\"):\n",
    "    features = extract_features(sequence)\n",
    "    test_feature_data.append(features)\n",
    "    test_entry_ids.append(entry_id)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_test_df = pd.DataFrame(test_feature_data, index=test_entry_ids)\n",
    "print(f\"Test features shape: {X_test_df.shape}\")\n",
    "\n",
    "# Standardize using training scaler\n",
    "X_test_scaled = scaler.transform(X_test_df.values)\n",
    "print(f\"Test features scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions for test set...\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    # Process in batches to manage GPU memory\n",
    "    batch_size = 10000\n",
    "    n_samples = len(X_test_scaled)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    test_pred_proba = np.zeros((n_samples, len(top_terms)))\n",
    "    \n",
    "    for i in tqdm(range(n_batches), desc=\"GPU prediction batches\"):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        X_batch_gpu = cp.asarray(X_test_scaled[start:end])\n",
    "        test_pred_proba[start:end] = cp.asnumpy(model.predict_proba(X_batch_gpu))\n",
    "        del X_batch_gpu\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "else:\n",
    "    test_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(f\"Predictions generated for {len(test_entry_ids)} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a51fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "for i, protein_id in enumerate(tqdm(test_entry_ids, desc=\"Building submission\")):\n",
    "    # Get predictions above threshold\n",
    "    probs = test_pred_proba[i]\n",
    "    pred_indices = np.where(probs >= PREDICTION_THRESHOLD)[0]\n",
    "    \n",
    "    if len(pred_indices) == 0:\n",
    "        # If no predictions above threshold, take top 5\n",
    "        pred_indices = np.argsort(probs)[-5:][::-1]\n",
    "    \n",
    "    for idx in pred_indices:\n",
    "        term = top_terms[idx]\n",
    "        score = probs[idx]\n",
    "        submission_rows.append({\n",
    "            'EntryID': protein_id,\n",
    "            'term': term,\n",
    "            'score': round(float(score), 4)\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "print(f\"\\nSubmission contains {len(submission_df)} predictions\")\n",
    "print(f\"Unique proteins: {submission_df['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {submission_df['term'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file\n",
    "submission_path = os.path.join(OUTPUT_DIR, 'submission.tsv')\n",
    "submission_df.to_csv(submission_path, sep='\\t', index=False, header=False)\n",
    "\n",
    "print(f\"\\n✓ Submission saved to: {submission_path}\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c866e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission statistics\n",
    "terms_per_protein = submission_df.groupby('EntryID').size()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUBMISSION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(f\"Unique proteins: {submission_df['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {submission_df['term'].nunique()}\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(f\"  Min:    {submission_df['score'].min():.4f}\")\n",
    "print(f\"  Max:    {submission_df['score'].max():.4f}\")\n",
    "print(f\"  Mean:   {submission_df['score'].mean():.4f}\")\n",
    "print(f\"  Median: {submission_df['score'].median():.4f}\")\n",
    "print(f\"\\nTerms per protein:\")\n",
    "print(f\"  Min:    {terms_per_protein.min()}\")\n",
    "print(f\"  Max:    {terms_per_protein.max()}\")\n",
    "print(f\"  Mean:   {terms_per_protein.mean():.1f}\")\n",
    "print(f\"  Median: {terms_per_protein.median():.1f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ff6d3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Performance:\n",
    "- **Algorithm**: Multi-label Logistic Regression (One-vs-Rest)\n",
    "- **Features**: Amino acid composition + physicochemical properties\n",
    "- **GO Terms**: Top 600 most common terms\n",
    "\n",
    "### Note:\n",
    "This is a baseline model using simple sequence features. For significantly better performance:\n",
    "1. Use the **KNN model with ESM-2 embeddings** (see `knn_protein_prediction.ipynb`)\n",
    "2. ESM-2 embeddings capture much richer protein representations\n",
    "3. Expected F1 improvement: ~2-3x better with ESM-2\n",
    "\n",
    "### Files Generated:\n",
    "- `logistic_regression_model.pkl` - Trained model\n",
    "- `submission.tsv` - Competition submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68055e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✓ All done! Ready for submission.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
