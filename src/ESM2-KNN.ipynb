{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-16T12:07:32.297921Z",
     "iopub.status.busy": "2025-12-16T12:07:32.296993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[io] train_terms proteins: 82,405\n",
      "[io] OBO parents nodes: 40,121\n",
      "[prep] Propagating train labels (cached ancestors)...\n",
      "[prep] chosen terms: 2,000\n",
      "[io] Loading train embeddings...\n",
      "[prep] Train proteins with labels: 82,404, X_train: (82404, 1280)\n",
      "[prep] Y_csr shape: (82404, 2000), nnz: 2,349,146\n",
      "[prep] Propagation edges (restricted): 2,065\n",
      "[train] Fitting KNN index...\n",
      "[train] KNN fitted in 0.05s\n",
      "[test] Loading test embeddings (mmap)...\n",
      "[test] N_test: 224,309, test_emb shape: (224309, 1280)\n",
      "[stream] 4,096/224,309 done\n",
      "[stream] 45,056/224,309 done\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORT\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. CONFIG\n",
    "# ----------------------------\n",
    "CONFIG = {\n",
    "    \"SAMPLE_SUBMISSION\": \"/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\",\n",
    "    \"TRAIN_TERMS\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n",
    "    \"GO_OBO\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\",\n",
    "    \"IA_FILE\": \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\",\n",
    "\n",
    "    \"TRAIN_EMBEDS\": \"/kaggle/input/embedding-esm2-650m/biggest embedding/train_embeddings_650M.npy\",\n",
    "    \"TRAIN_IDS\": \"/kaggle/input/embedding-esm2-650m/biggest embedding/train_ids.npy\",\n",
    "    \"TEST_EMBEDS\": \"/kaggle/input/embedding-esm2-650m/biggest embedding/test_embeddings_650M.npy\",\n",
    "    \"TEST_IDS\": \"/kaggle/input/embedding-esm2-650m/biggest embedding/test_ids.npy\",\n",
    "\n",
    "    \"OUTPUT_SUBMISSION\": \"/kaggle/working/submission.tsv\",\n",
    "\n",
    "    # KNN\n",
    "    \"KNN_K\": 50,\n",
    "    \"KNN_METRIC\": \"cosine\",\n",
    "    \"KNN_SIGMA\": 0.15,\n",
    "\n",
    "    # Label space\n",
    "    \"TOP_K_LABELS\": 2000,\n",
    "\n",
    "    # Inference\n",
    "    \"PREDICT_BATCH_SIZE\": 4096,     \n",
    "    \"TOP_K_PER_PROTEIN\": 150,\n",
    "    \"MIN_SCORE\": 0.001,            \n",
    "\n",
    "    # Propagation (optional)\n",
    "    \"PROPAGATE_TRAIN_LABELS\": True,\n",
    "    \"PROPAGATE_PREDICTIONS\": True,\n",
    "    \"PROP_PASSES\": 2,\n",
    "\n",
    "    \"RANDOM_SEED\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. HÀM HỖ TRỢ\n",
    "# ----------------------------\n",
    "def clean_ids(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert bytes->str and split 'tax|ID' -> 'ID' if needed.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    out = []\n",
    "    for x in arr:\n",
    "        if isinstance(x, (bytes, np.bytes_)):\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            x = str(x)\n",
    "        if \"|\" in x:\n",
    "            parts = x.split(\"|\")\n",
    "            if len(parts) >= 2:\n",
    "                x = parts[1]\n",
    "        out.append(x)\n",
    "    return np.array(out, dtype=object)\n",
    "\n",
    "def l2_normalize_inplace(X: np.ndarray, chunk: int = 8192) -> None:\n",
    "    \"\"\"In-place L2 normalize rows (float32).\"\"\"\n",
    "    for i in range(0, X.shape[0], chunk):\n",
    "        sub = X[i:i+chunk]\n",
    "        norms = np.linalg.norm(sub, axis=1, keepdims=True)\n",
    "        sub /= np.maximum(norms, 1e-12)\n",
    "\n",
    "def l2_normalize_copy(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / np.maximum(norms, 1e-12)\n",
    "\n",
    "def read_train_terms_fast(path: str) -> dict:\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"protein\", \"go\", \"ont\"], dtype=str)\n",
    "    mapping = df.groupby(\"protein\")[\"go\"].apply(list).to_dict()\n",
    "    print(f\"[io] train_terms proteins: {len(mapping):,}\")\n",
    "    return mapping\n",
    "\n",
    "def read_ia(path: str) -> dict:\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"go\", \"score\"])\n",
    "    df[\"score\"] = df[\"score\"].astype(float)\n",
    "    return dict(zip(df[\"go\"].values, df[\"score\"].values))\n",
    "\n",
    "def parse_obo_parents(go_obo_path: str) -> dict:\n",
    "    parents = defaultdict(set)\n",
    "    if not os.path.exists(go_obo_path):\n",
    "        return parents\n",
    "    cur_id = None\n",
    "    with open(go_obo_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"[Term]\":\n",
    "                cur_id = None\n",
    "            elif line.startswith(\"id: \"):\n",
    "                cur_id = line.split(\"id: \")[1].strip()\n",
    "            elif line.startswith(\"is_a: \"):\n",
    "                pid = line.split()[1].strip()\n",
    "                if cur_id:\n",
    "                    parents[cur_id].add(pid)\n",
    "            elif line.startswith(\"relationship: part_of \"):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3 and cur_id:\n",
    "                    parents[cur_id].add(parts[2].strip())\n",
    "    print(f\"[io] OBO parents nodes: {len(parents):,}\")\n",
    "    return parents\n",
    "\n",
    "def build_edges(parents_map: dict, classes: np.ndarray) -> list:\n",
    "    term_to_idx = {t: i for i, t in enumerate(classes)}\n",
    "    edges = []\n",
    "    for child, ps in parents_map.items():\n",
    "        if child not in term_to_idx:\n",
    "            continue\n",
    "        c = term_to_idx[child]\n",
    "        for p in ps:\n",
    "            if p in term_to_idx:\n",
    "                edges.append((c, term_to_idx[p]))\n",
    "    return edges\n",
    "\n",
    "def propagate_dense(scores: np.ndarray, edges: list, passes: int = 2) -> np.ndarray:\n",
    "    # scores: (B, C)\n",
    "    for _ in range(passes):\n",
    "        for c, p in edges:\n",
    "            # parent = max(parent, child)\n",
    "            np.maximum(scores[:, p], scores[:, c], out=scores[:, p])\n",
    "    return scores\n",
    "\n",
    "def choose_top_terms(train_terms: dict, ia_dict: dict, top_k: int) -> list:\n",
    "    freq = Counter()\n",
    "    for _, terms in train_terms.items():\n",
    "        freq.update(terms)\n",
    "\n",
    "    # IA-weighted frequency\n",
    "    scored = []\n",
    "    for t, f in freq.items():\n",
    "        scored.append((t, float(ia_dict.get(t, 0.0)) * f))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    chosen = [t for t, _ in scored[:top_k]]\n",
    "    print(f\"[prep] chosen terms: {len(chosen):,}\")\n",
    "    return chosen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Sparse weighted voting\n",
    "# ----------------------------\n",
    "def knn_weighted_vote_sparse(Y_csr: sp.csr_matrix,\n",
    "                            neighbor_idx: np.ndarray,\n",
    "                            dists: np.ndarray,\n",
    "                            sigma: float) -> sp.csr_matrix:\n",
    "    \"\"\"\n",
    "    Y_csr: (N_train, C) sparse labels\n",
    "    neighbor_idx: (B, K) indices into train\n",
    "    dists: (B, K) cosine distances\n",
    "    returns scores CSR (B, C) where each row is weighted average of neighbors' labels\n",
    "    \"\"\"\n",
    "    B, K = neighbor_idx.shape\n",
    "    w = np.exp(-dists / sigma).astype(np.float32)\n",
    "    w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    idx_flat = neighbor_idx.reshape(-1)\n",
    "    w_flat = w.reshape(-1)\n",
    "\n",
    "    # Gather neighbor label rows: (B*K, C)\n",
    "    Y_nb = Y_csr[idx_flat]                 # CSR\n",
    "    Y_nb = Y_nb.multiply(w_flat[:, None])  # scale rows by weights (no huge dense tensor)\n",
    "\n",
    "    # Group-sum every K rows -> B rows using a CSR \"grouping\" matrix\n",
    "    # Each output row sums its corresponding block of K rows\n",
    "    indptr = np.arange(0, B * K + 1, K, dtype=np.int32)\n",
    "    indices = np.arange(B * K, dtype=np.int32)\n",
    "    data = np.ones(B * K, dtype=np.float32)\n",
    "    G = sp.csr_matrix((data, indices, indptr), shape=(B, B * K))\n",
    "\n",
    "    scores = G @ Y_nb  # (B, C) CSR\n",
    "    return scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. LOAD TRAIN TERMS + OBO + IA\n",
    "# ============================================================\n",
    "train_terms = read_train_terms_fast(CONFIG[\"TRAIN_TERMS\"])\n",
    "parents_map = parse_obo_parents(CONFIG[\"GO_OBO\"])\n",
    "ia_dict = read_ia(CONFIG[\"IA_FILE\"])\n",
    "\n",
    "# Optional: propagate train labels up GO graph (ancestor closure)\n",
    "if CONFIG[\"PROPAGATE_TRAIN_LABELS\"] and len(parents_map) > 0:\n",
    "    print(\"[prep] Propagating train labels (cached ancestors)...\")\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def ancestors(term: str) -> tuple:\n",
    "        stack = [term]\n",
    "        seen = set()\n",
    "        while stack:\n",
    "            cur = stack.pop()\n",
    "            for p in parents_map.get(cur, ()):\n",
    "                if p not in seen:\n",
    "                    seen.add(p)\n",
    "                    stack.append(p)\n",
    "        return tuple(seen)\n",
    "\n",
    "    for p, terms in train_terms.items():\n",
    "        s = set(terms)\n",
    "        for t in list(s):\n",
    "            s.update(ancestors(t))\n",
    "        train_terms[p] = list(s)\n",
    "\n",
    "# Choose label space\n",
    "chosen_terms = choose_top_terms(train_terms, ia_dict, CONFIG[\"TOP_K_LABELS\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. LOAD TRAIN EMBEDDINGS + FILTER + NORMALIZE\n",
    "# ============================================================\n",
    "print(\"[io] Loading train embeddings...\")\n",
    "train_emb_full = np.load(CONFIG[\"TRAIN_EMBEDS\"])  # usually fits RAM\n",
    "train_ids_full = clean_ids(np.load(CONFIG[\"TRAIN_IDS\"]))\n",
    "\n",
    "# Filter to proteins that have labels\n",
    "valid_mask = np.array([pid in train_terms for pid in train_ids_full], dtype=bool)\n",
    "valid_indices = np.where(valid_mask)[0]\n",
    "X_train = np.asarray(train_emb_full[valid_indices], dtype=np.float32)  # copy filtered to RAM\n",
    "train_pids = train_ids_full[valid_indices].tolist()\n",
    "\n",
    "del train_emb_full, train_ids_full, valid_mask\n",
    "gc.collect()\n",
    "\n",
    "print(f\"[prep] Train proteins with labels: {len(train_pids):,}, X_train: {X_train.shape}\")\n",
    "\n",
    "# Normalize train embeddings\n",
    "l2_normalize_inplace(X_train)\n",
    "gc.collect()\n",
    "\n",
    "# Build sparse label matrix (CSR)\n",
    "y_labels = [[t for t in train_terms[pid] if t in set(chosen_terms)] for pid in train_pids]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=chosen_terms, sparse_output=True)\n",
    "Y_csr = mlb.fit_transform(y_labels).tocsr().astype(np.float32)\n",
    "classes = mlb.classes_\n",
    "\n",
    "print(f\"[prep] Y_csr shape: {Y_csr.shape}, nnz: {Y_csr.nnz:,}\")\n",
    "\n",
    "# Build propagation edges restricted to chosen terms\n",
    "edges = []\n",
    "if CONFIG[\"PROPAGATE_PREDICTIONS\"] and len(parents_map) > 0:\n",
    "    edges = build_edges(parents_map, classes)\n",
    "    print(f\"[prep] Propagation edges (restricted): {len(edges):,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. FIT KNN (cosine, brute)\n",
    "# ============================================================\n",
    "print(\"[train] Fitting KNN index...\")\n",
    "knn = NearestNeighbors(\n",
    "    n_neighbors=CONFIG[\"KNN_K\"],\n",
    "    metric=CONFIG[\"KNN_METRIC\"],\n",
    "    algorithm=\"brute\",      # good for cosine + high-dim embeddings\n",
    "    n_jobs=-1\n",
    ")\n",
    "t0 = time.time()\n",
    "knn.fit(X_train)\n",
    "print(f\"[train] KNN fitted in {time.time() - t0:.2f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. INFERENCE ON TEST (memmap) + WRITE SUBMISSION\n",
    "# ============================================================\n",
    "print(\"[test] Loading test embeddings (mmap)...\")\n",
    "test_emb = np.load(CONFIG[\"TEST_EMBEDS\"], mmap_mode=\"r\")\n",
    "test_ids = clean_ids(np.load(CONFIG[\"TEST_IDS\"]))\n",
    "N_test = len(test_ids)\n",
    "print(f\"[test] N_test: {N_test:,}, test_emb shape: {test_emb.shape}\")\n",
    "\n",
    "# Detect header from sample submission\n",
    "sample_path = CONFIG[\"SAMPLE_SUBMISSION\"]\n",
    "write_header = False\n",
    "header_cols = None\n",
    "try:\n",
    "    sample_df = pd.read_csv(sample_path, sep=\"\\t\", nrows=5)\n",
    "    header_cols = list(sample_df.columns)\n",
    "    # If sample has 3 columns and looks like header, we'll write it.\n",
    "    # (If Kaggle expects no header, set write_header=False)\n",
    "    write_header = True\n",
    "except:\n",
    "    write_header = False\n",
    "\n",
    "out_path = CONFIG[\"OUTPUT_SUBMISSION\"]\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    if write_header and header_cols is not None and len(header_cols) >= 3:\n",
    "        f.write(\"\\t\".join(header_cols[:3]) + \"\\n\")\n",
    "\n",
    "    B = CONFIG[\"PREDICT_BATCH_SIZE\"]\n",
    "    K = CONFIG[\"KNN_K\"]\n",
    "    sigma = float(CONFIG[\"KNN_SIGMA\"])\n",
    "    topk = int(CONFIG[\"TOP_K_PER_PROTEIN\"])\n",
    "    min_score = float(CONFIG[\"MIN_SCORE\"])\n",
    "\n",
    "    for start in range(0, N_test, B):\n",
    "        end = min(start + B, N_test)\n",
    "\n",
    "        Xb = np.asarray(test_emb[start:end], dtype=np.float32)\n",
    "        Xb = l2_normalize_copy(Xb)\n",
    "\n",
    "        # neighbors in train\n",
    "        dists, idxs = knn.kneighbors(Xb, n_neighbors=K)\n",
    "\n",
    "        # sparse voting\n",
    "        scores_csr = knn_weighted_vote_sparse(Y_csr, idxs, dists, sigma=sigma)\n",
    "\n",
    "        # convert to dense for fast top-k\n",
    "        scores = scores_csr.toarray().astype(np.float32, copy=False)\n",
    "\n",
    "        # optional propagation on dense\n",
    "        if edges:\n",
    "            scores = propagate_dense(scores, edges, passes=int(CONFIG[\"PROP_PASSES\"]))\n",
    "\n",
    "        # Top-k indices per row (vectorized)\n",
    "        k_eff = min(topk, scores.shape[1])\n",
    "        top_idx = np.argpartition(scores, -k_eff, axis=1)[:, -k_eff:]\n",
    "        top_scores = np.take_along_axis(scores, top_idx, axis=1)\n",
    "        order = np.argsort(top_scores, axis=1)[:, ::-1]\n",
    "        top_idx = np.take_along_axis(top_idx, order, axis=1)\n",
    "        top_scores = np.take_along_axis(top_scores, order, axis=1)\n",
    "\n",
    "        # Write lines (buffered)\n",
    "        lines = []\n",
    "        ids_batch = test_ids[start:end]\n",
    "        for i, pid in enumerate(ids_batch):\n",
    "            # filter by min_score\n",
    "            mask = top_scores[i] >= min_score\n",
    "            if not np.any(mask):\n",
    "                # ensure at least 1 prediction\n",
    "                j = int(top_idx[i, 0])\n",
    "                lines.append(f\"{pid}\\t{classes[j]}\\t{float(top_scores[i,0]):.3f}\\n\")\n",
    "                continue\n",
    "\n",
    "            for j, sc in zip(top_idx[i, mask], top_scores[i, mask]):\n",
    "                lines.append(f\"{pid}\\t{classes[int(j)]}\\t{float(sc):.3f}\\n\")\n",
    "\n",
    "        f.write(\"\".join(lines))\n",
    "\n",
    "        if (start // B) % 10 == 0:\n",
    "            print(f\"[stream] {end:,}/{N_test:,} done\")\n",
    "\n",
    "print(f\"[done] Wrote submission: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 9014025,
     "sourceId": 14144139,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
