{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"},{"sourceId":14144139,"sourceType":"datasetVersion","datasetId":9014025}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ------------------------------------------------------------\n# imports\n# ------------------------------------------------------------\nimport os, gc, math, random, sys, time\nfrom collections import defaultdict, Counter\nfrom typing import List, Dict, Tuple, Set\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\nfrom collections import defaultdict\n\nprint(\"Import done!!!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T08:14:29.605519Z","iopub.execute_input":"2025-12-16T08:14:29.606028Z","iopub.status.idle":"2025-12-16T08:14:29.612045Z","shell.execute_reply.started":"2025-12-16T08:14:29.606004Z","shell.execute_reply":"2025-12-16T08:14:29.611249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# TSV FILES\n# ------------------------------------------------------------\nSAMPLE_SUBMISSION_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\"\nIA_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"\nTRAIN_TERMS_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_TAXONOMY_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\n\n# FASTA FILES (Vẫn giữ để tham khảo nếu cần, nhưng logic chính dùng npy)\nTESTSUPERSET_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"\nTRAIN_SEQUENCES_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\n\n# OBO FILE\nGO_BASIC_OBO = \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n\n# PRE-COMPUTED EMBEDDINGS (NEW)\nTRAIN_EMBEDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/train_embeddings_650M.npy\"\nTRAIN_IDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/train_ids.npy\"\nTEST_EMBEDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/test_embeddings_650M.npy\"\nTEST_IDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/test_ids.npy\"\n\n# OUTPUT FILE\nOUTPUT_TSV = \"/kaggle/working/submission.tsv\"\n\nprint(\"Files are listed!!!\")\n\n# ------------------------------------------------------------\n# CONFIG\n# ------------------------------------------------------------\nCONFIG = {\n    \"TRAIN_TERMS\": TRAIN_TERMS_TSV,\n    \"GO_OBO\": GO_BASIC_OBO,\n    \"IA_FILE\": IA_TSV,\n    \"OUTPUT_SUBMISSION\": OUTPUT_TSV,\n    \n    # Pre-computed paths\n    \"TRAIN_EMBEDS\": TRAIN_EMBEDS_NPY,\n    \"TRAIN_IDS\": TRAIN_IDS_NPY,\n    \"TEST_EMBEDS\": TEST_EMBEDS_NPY,\n    \"TEST_IDS\": TEST_IDS_NPY,\n\n    \"PREDICT_BATCH_SIZE\": 4096,\n    \n    \n    \n    # Model training hyperparams\n    \"RANDOM_SEED\": 42,\n    \n\n    \"BATCH_SIZE\": 256,\n    \"TOP_K_LABELS\": 2000,\n    \"EPOCHS\": 70,\n    \"LEARNING_RATE\": 5e-4,\n    \"HIDDEN_UNITS\": 2048,\n    \"DROPOUT\": 0.2,\n    \n    # Submission postprocessing\n    \"TOP_K_PER_PROTEIN\": 150,\n    \"GLOBAL_THRESHOLD_SEARCH\": True,\n    \"THRESHOLD_GRID\": [i/100 for i in range(1, 51)],\n    \n    # Propagation\n    \"PROPAGATE_TRAIN_LABELS\": True,\n    \"PROPAGATE_PREDICTIONS\": True,\n}\n\nprint(\"Config loaded...\")\n\n# deterministic seeds\nrandom.seed(CONFIG[\"RANDOM_SEED\"])\nnp.random.seed(CONFIG[\"RANDOM_SEED\"])\ntf.random.set_seed(CONFIG[\"RANDOM_SEED\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Utils for TSV/OBO parsing \n# ------------------------------------------------------------\ndef read_train_terms(path):\n    # 1. Đọc file \n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"protein\", \"go\", \"ont\"], dtype=str)\n    \n    # 2. Tối ưu hóa tốc độ \n    mapping = df.groupby(\"protein\")[\"go\"].apply(list).to_dict()\n    \n    print(f\"[io] Read training annotations for {len(mapping)} proteins from {path}\")\n    return mapping\n\ndef parse_obo(go_obo_path: str) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]:\n    parents = defaultdict(set); children = defaultdict(set)\n    if not os.path.exists(go_obo_path): return parents, children\n    with open(go_obo_path,\"r\") as f:\n        cur_id=None\n        for line in f:\n            line=line.strip()\n            if line==\"[Term]\": cur_id=None\n            elif line.startswith(\"id: \"): cur_id=line.split(\"id: \")[1].strip()\n            elif line.startswith(\"is_a: \"):\n                pid=line.split()[1].strip()\n                if cur_id: parents[cur_id].add(pid); children[pid].add(cur_id)\n            elif line.startswith(\"relationship: part_of \"):\n                parts=line.split(); \n                if len(parts)>=3:\n                    pid=parts[2].strip()\n                    if cur_id: parents[cur_id].add(pid); children[pid].add(cur_id)\n    print(f\"[io] Parsed OBO: {len(parents)} nodes with parents\")\n    return parents, children\n\ndef get_ancestors(go_id: str, parents: Dict[str, Set[str]]) -> Set[str]:\n    ans=set(); stack=[go_id]\n    while stack:\n        cur=stack.pop()\n        for p in parents.get(cur,[]): \n            if p not in ans:\n                ans.add(p); stack.append(p)\n    return ans\n\ndef read_IA(path: str) -> pd.DataFrame:\n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"go\", \"score\"])\n    df[\"score\"] = df[\"score\"].astype(float)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Load Metadata (Terms, OBO, IA)\n# ------------------------------------------------------------\ntrain_terms = read_train_terms(CONFIG[\"TRAIN_TERMS\"])\nparents_map, children_map = parse_obo(CONFIG[\"GO_OBO\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Load PRE-COMPUTED TRAIN EMBEDDINGS\n# ------------------------------------------------------------\nprint(\"[io] Loading pre-computed TRAIN embeddings...\")\n# Load train embeddings and IDs\ntrain_emb_full = np.load(CONFIG[\"TRAIN_EMBEDS\"])\ntrain_ids_full = np.load(CONFIG[\"TRAIN_IDS\"])\nprint(\"ID trước khi sửa:\", train_ids_full[0])\ntrain_ids_full = pd.Series(train_ids_full).apply(lambda x: x.split('|')[1]).values\nprint(\"ID sau khi sửa:\", train_ids_full[0])\nprint(f\"[io] Loaded train embeddings shape: {train_emb_full.shape}\")\nprint(f\"[io] Loaded train ids shape: {train_ids_full.shape}\")\n\n# Filter: Chỉ giữ lại các protein có trong file train_terms\n# (Đôi khi file embedding chứa nhiều protein hơn hoặc file terms chứa nhiều hơn, cần giao nhau)\nvalid_indices = []\nvalid_ids = []\nfor idx, pid in enumerate(train_ids_full):\n    if pid in train_terms:\n        valid_indices.append(idx)\n        valid_ids.append(pid)\n\nX_emb = train_emb_full[valid_indices]\nX_proteins = valid_ids\nprint(f\"[prep] Filtered to {len(X_proteins)} proteins that have labels.\")\n\n# Giải phóng bộ nhớ array gốc nếu cần\ndel train_emb_full, train_ids_full\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Label Processing (IA Scores & Propagation)\n# ------------------------------------------------------------\n# Propagate train labels\nif CONFIG[\"PROPAGATE_TRAIN_LABELS\"] and parents_map:\n    print(\"[prep] Propagating train labels up GO graph\")\n    propagated={}\n    for p in X_proteins:\n        terms=set(train_terms[p])\n        extra=set()\n        for t in list(terms): extra |= get_ancestors(t, parents_map)\n        propagated[p]=sorted(terms|extra)\n    # Cập nhật lại train_terms với bản đã propagate cho các protein dùng để train\n    for p in X_proteins:\n        train_terms[p] = propagated[p]\n\nIA_list = read_IA(CONFIG[\"IA_FILE\"])\nterm_freq = Counter()\nfor p in X_proteins:\n    term_freq.update(train_terms[p])\n\nIA_dict = dict(zip(IA_list[\"go\"], IA_list[\"score\"]))\nterm_combined_score = {}\nfor t in term_freq:\n    ia_score = IA_dict.get(t, 0)\n    term_combined_score[t] = ia_score * term_freq[t]\n\nif CONFIG[\"TOP_K_LABELS\"] is not None:\n    top_terms_sorted = sorted(term_combined_score.items(), key=lambda x: x[1], reverse=True)\n    chosen_terms = set([t for t, _ in top_terms_sorted[:CONFIG[\"TOP_K_LABELS\"]]])\n    print(f\"[prep] Restricting to top-{CONFIG['TOP_K_LABELS']} GO terms\")\nelse:\n    chosen_terms = set(term_combined_score.keys())\n\n# Chuẩn bị Y\ny_labels = [[t for t in train_terms[p] if t in chosen_terms] for p in X_proteins]\nmlb = MultiLabelBinarizer(classes=sorted(chosen_terms))\nY = mlb.fit_transform(y_labels).astype(np.float32)\nprint(\"[prep] Label matrix shape:\", Y.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Train / validation split\n# ------------------------------------------------------------\n# X_emb đã được load ở trên\ny = Y\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_emb, y, test_size=0.1, random_state=CONFIG[\"RANDOM_SEED\"]\n)\nprint(\"[train] shapes:\", X_tr.shape, X_val.shape, y_tr.shape, y_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# ĐỊNH NGHĨA HÀM FOCAL LOSS\n# ------------------------------------------------------------\ndef focal_loss_fixed(gamma, alpha):\n    def focal_loss_fn(y_true, y_pred):\n        # Ép kiểu dữ liệu về float32 để tránh lỗi\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        \n        # Clip giá trị để tránh log(0) gây ra NaN\n        epsilon = tf.keras.backend.epsilon()\n        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n        \n        # Tính Focal Loss\n        # Công thức: -alpha * (1 - p)^gamma * log(p)\n        # alpha=0.25 giúp giảm ảnh hưởng của nhãn âm (chiếm đa số)\n        loss = alpha * tf.pow(1 - y_pred, gamma) * y_true * tf.math.log(y_pred) \\\n             + (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true) * tf.math.log(1 - y_pred)\n             \n        # Trả về giá trị dương (Minimize loss)\n        return -tf.reduce_mean(loss, axis=-1)\n        \n    return focal_loss_fn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# ResNet MLP\n# ------------------------------------------------------------\ndef build_resnet_model(input_dim, output_dim, hidden_units, dropout, lr, n_blocks):\n    # --- Input ---\n    inputs = layers.Input(shape=(input_dim,))\n\n    # --- Projection Layer ---\n    # Projection này giúp đưa embedding về chiều không gian của hidden_units\n    x = layers.Dense(hidden_units, activation=\"gelu\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout)(x)\n\n    # --- Residual Blocks ---\n    for i in range(n_blocks):\n        # A. Lưu shortcut\n        shortcut = x\n        \n        # B. Main Path\n        # Layer 1\n        res = layers.Dense(hidden_units, activation=\"gelu\")(x)\n        res = layers.BatchNormalization()(res)\n        res = layers.Dropout(dropout)(res)\n        \n        # Layer 2: QUAN TRỌNG - Activation linear để cộng với shortcut\n        res = layers.Dense(hidden_units, activation=\"gelu\")(res)\n        res = layers.BatchNormalization()(res)\n        \n        # C. Add (ResNet Connection)\n        # \n        x = layers.Add()([x, res])\n        \n        # D. Activation sau khi cộng\n        x = layers.Activation('gelu')(x) # 0.273 voi relu\n\n    # --- Output Layer ---\n    # Multi-label classification -> dùng Sigmoid\n    outputs = layers.Dense(output_dim, activation=\"sigmoid\")(x) \n\n    # --- Tạo Model Object ---\n    model = models.Model(inputs=inputs, outputs=outputs, name=\"ResNet_MLP\")\n\n    # --- 1. Định nghĩa Schedule TRƯỚC (Để nhét vào optimizer) ---\n    # Lưu ý: X_tr và CONFIG phải là biến toàn cục (global) hoặc được truyền vào hàm\n    steps_per_epoch = len(X_tr) // CONFIG[\"BATCH_SIZE\"]\n    \n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-3,             # Bắt đầu\n        decay_steps=CONFIG[\"EPOCHS\"] * steps_per_epoch, # Giảm dần\n        alpha=0.01                              # Kết thúc ở 1% LR ban đầu\n    )\n\n    # --- 2. Khởi tạo Optimizer VỚI Schedule ---\n    try:\n        # TF 2.11+ (AdamW chuẩn)\n        # QUAN TRỌNG: learning_rate=lr_schedule (Không dùng số cứng nữa)\n        opt = optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-2) #**************************************0.275 la 1e-3 (esm 2 150M)\n    except AttributeError:\n        # Fallback cho TF cũ\n        opt = optimizers.Adam(learning_rate=lr_schedule)\n\n    # --- 3. Compile ---\n    model.compile(\n        optimizer=opt,\n        loss=focal_loss_fixed(gamma=2.0, alpha=0.35), # tang tu 0.25 len 0.35*********************************\n        metrics=[metrics.BinaryAccuracy(name='acc'), metrics.AUC(name='auc')]\n    )\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# 3. Khởi tạo và Run\n# ------------------------------------------------------------\n# Giả lập shape dữ liệu\nD = X_tr.shape[1] \nM = y_tr.shape[1]\n\nmodel = build_resnet_model(\n    input_dim=D, \n    output_dim=CONFIG[\"TOP_K_LABELS\"],\n    hidden_units=CONFIG[\"HIDDEN_UNITS\"],\n    dropout=CONFIG[\"DROPOUT\"],\n    lr=CONFIG[\"LEARNING_RATE\"],\n    n_blocks=2\n)\n\nmodel.summary()\n\n# Checkpoint & Early Stopping\nckpt_path = \"/kaggle/working/best_resnet_model.keras\" # Đuôi .keras là chuẩn mới của TF\nes = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1)\nmc = callbacks.ModelCheckpoint(\"/kaggle/working/best_resnet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Train \n# ------------------------------------------------------------\n\nrlr = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5,   \n    patience=3,     \n    verbose=1, \n    min_lr=1e-6\n)\n\n# 4. Train\nhistory = model.fit(\n    X_tr, y_tr, \n    validation_data=(X_val, y_val),\n    epochs=CONFIG[\"EPOCHS\"], \n    batch_size=CONFIG[\"BATCH_SIZE\"],\n    callbacks=[es, mc, rlr],\n    verbose=2,\n    shuffle=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Evaluate & select global threshold\n# ------------------------------------------------------------\ndef read_IA_safe(path):\n    if not os.path.exists(path): return {}\n    df=pd.read_csv(path, sep=\"\\t\", header=None, names=[\"go\",\"ia\"], dtype=str)\n    d={}\n    for _,r in df.iterrows():\n        try: d[r.go]=float(r.ia)\n        except: \n            try: d[r.go]=float(r.ia.replace(\",\",\".\")) \n            except: d[r.go]=0.0\n    return d\n\nia_weights = read_IA_safe(CONFIG[\"IA_FILE\"])\n\ndef weighted_precision_recall_f1(y_true, y_pred_bin, mlb_obj):\n    # Simplified version using globals/closures\n    tp = ((y_true==1)&(y_pred_bin==1)).sum(axis=0).astype(float)\n    fp = ((y_true==0)&(y_pred_bin==1)).sum(axis=0).astype(float)\n    fn = ((y_true==1)&(y_pred_bin==0)).sum(axis=0).astype(float)\n    eps=1e-12\n    prec = tp/(tp+fp+eps); rec = tp/(tp+fn+eps)\n    f1 = 2*prec*rec/(prec+rec+eps)\n    cls = mlb_obj.classes_\n    weights = np.array([ia_weights.get(c,1.0) for c in cls], dtype=float)\n    weighted_f1 = (f1*weights).sum()/(weights.sum()+eps)\n    return weighted_f1\n\ny_val_prob = model.predict(X_val, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\nbest_thresh = 0.5; best_score = -1.0\nif CONFIG[\"GLOBAL_THRESHOLD_SEARCH\"]:\n    for t in CONFIG[\"THRESHOLD_GRID\"]:\n        y_pred_bin = (y_val_prob >= t).astype(int)\n        f1 = weighted_precision_recall_f1(y_val, y_pred_bin, mlb)\n        if f1 > best_score: best_score=f1; best_thresh=t\nprint(f\"[eval] best_thresh {best_thresh} best IA-weighted F1 {best_score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# Inference on TEST set using Pre-computed Embeddings\n# ------------------------------------------------------------\nprint(\"[test] Loading pre-computed TEST embeddings...\")\ntest_emb_full = np.load(CONFIG[\"TEST_EMBEDS\"]) \ntest_ids_full = np.load(CONFIG[\"TEST_IDS\"])\nN_test = len(test_ids_full)\nprint(f\"[test] Loaded {N_test} test embeddings. Shape: {test_emb_full.shape}\")\n\n# Precompute helpful mappings for propagation\nterm_to_idx = {t:i for i,t in enumerate(mlb.classes_)}\nrestricted_parents = {}\nfor t in mlb.classes_:\n    restricted_parents[t] = set([p for p in parents_map.get(t, set()) if p in term_to_idx])\n\ndef propagate_batch(pred_batch: np.ndarray, parents_map_local: Dict[str, Set[str]], classes_list: List[str], iterations=3):\n    B, Mloc = pred_batch.shape\n    idx_map = {i:classes_list[i] for i in range(Mloc)}\n    term_to_idx_local = {classes_list[i]: i for i in range(Mloc)}\n    for _ in range(iterations):\n        changed = False\n        for child_idx in range(Mloc):\n            child_term = idx_map[child_idx]\n            child_scores = pred_batch[:, child_idx]\n            for pterm in parents_map_local.get(child_term, []):\n                pidx = term_to_idx_local[pterm]\n                mask = child_scores > pred_batch[:, pidx]\n                if mask.any():\n                    pred_batch[mask, pidx] = child_scores[mask]\n                    changed = True\n        if not changed: break\n    return pred_batch\n\n# Open submission file\nout_fpath = CONFIG[\"OUTPUT_SUBMISSION\"]\nopen(out_fpath, \"w\").close() \nout_f = open(out_fpath, \"a\")\n\nprint(f\"[test] Predicting in batches of {CONFIG['PREDICT_BATCH_SIZE']}...\")\n\n# Iterate directly over the loaded arrays\nfor i in range(0, N_test, CONFIG[\"PREDICT_BATCH_SIZE\"]):\n    # Slice batch\n    X_batch = test_emb_full[i : i + CONFIG[\"PREDICT_BATCH_SIZE\"]]\n    ids_batch = test_ids_full[i : i + CONFIG[\"PREDICT_BATCH_SIZE\"]]\n    \n    # Predict\n    y_batch_prob = model.predict(X_batch, batch_size=min(128, len(X_batch)), verbose=0)\n    \n    # Propagate\n    if CONFIG[\"PROPAGATE_PREDICTIONS\"] and parents_map:\n        y_batch_prob = propagate_batch(y_batch_prob, restricted_parents, list(mlb.classes_), iterations=3)\n        \n    # Write submission\n    for ridx, pid in enumerate(ids_batch):\n        probs = y_batch_prob[ridx]\n        \n        # Filter top-k\n        #top_k = CONFIG[\"TOP_K_PER_PROTEIN\"]\n        top_k = 100\n        idxs = np.argsort(probs)[-top_k:]\n        idxs = [int(x) for x in idxs if probs[x] > 1e-3]\n        idxs = sorted(idxs, key=lambda x: probs[x], reverse=True)\n        \n        # Filter by threshold (optional logic combined)\n        # idxs = [x for x in idxs if probs[x] >= best_thresh] # Nếu muốn dùng threshold cứng\n\n        for idx in idxs:\n            score = float(probs[idx])\n            go_id = mlb.classes_[idx]\n            out_f.write(f\"{pid}\\t{go_id}\\t{score:.3f}\\n\")\n            \n    if (i // CONFIG[\"PREDICT_BATCH_SIZE\"]) % 10 == 0:\n        out_f.flush()\n        print(f\"[stream] processed {min(i + CONFIG['PREDICT_BATCH_SIZE'], N_test)} / {N_test}\")\n\nout_f.close()\nprint(f\"[done] Submission written to {CONFIG['OUTPUT_SUBMISSION']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}