{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-16T08:14:29.606028Z",
     "iopub.status.busy": "2025-12-16T08:14:29.605519Z",
     "iopub.status.idle": "2025-12-16T08:14:29.612045Z",
     "shell.execute_reply": "2025-12-16T08:14:29.611249Z",
     "shell.execute_reply.started": "2025-12-16T08:14:29.606004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Imports\n",
    "# ------------------------------------------------------------\n",
    "import os, gc, math, random, sys, time\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Import done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. TSV FILES\n",
    "# ------------------------------------------------------------\n",
    "# CẤU HÌNH ĐƯỜNG DẪN\n",
    "SAMPLE_SUBMISSION_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\"\n",
    "IA_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"\n",
    "TRAIN_TERMS_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\n",
    "TRAIN_TAXONOMY_TSV = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\n",
    "TESTSUPERSET_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"\n",
    "TRAIN_SEQUENCES_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\n",
    "\n",
    "# OBO FILE\n",
    "GO_BASIC_OBO = \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n",
    "\n",
    "# PRE-COMPUTED EMBEDDINGS \n",
    "TRAIN_EMBEDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/train_embeddings_650M.npy\"\n",
    "TRAIN_IDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/train_ids.npy\"\n",
    "TEST_EMBEDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/test_embeddings_650M.npy\"\n",
    "TEST_IDS_NPY = \"/kaggle/input/embedding-esm2-650m/biggest embedding/test_ids.npy\"\n",
    "\n",
    "# OUTPUT FILE\n",
    "OUTPUT_TSV = \"/kaggle/working/submission.tsv\"\n",
    "\n",
    "print(\"Files are listed!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. CONFIG\n",
    "# ------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    \"TRAIN_TERMS\": TRAIN_TERMS_TSV,\n",
    "    \"GO_OBO\": GO_BASIC_OBO,\n",
    "    \"IA_FILE\": IA_TSV,\n",
    "    \"OUTPUT_SUBMISSION\": OUTPUT_TSV,\n",
    "    \"TRAIN_EMBEDS\": TRAIN_EMBEDS_NPY,\n",
    "    \"TRAIN_IDS\": TRAIN_IDS_NPY,\n",
    "    \"TEST_EMBEDS\": TEST_EMBEDS_NPY,\n",
    "    \"TEST_IDS\": TEST_IDS_NPY,\n",
    "\n",
    "    \"PREDICT_BATCH_SIZE\": 4096,\n",
    "    \n",
    "    \n",
    "    \n",
    "    # THAM SỐ MODEL\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \n",
    "\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"TOP_K_LABELS\": 2000,\n",
    "    \"EPOCHS\": 70,\n",
    "    \"LEARNING_RATE\": 5e-4,\n",
    "    \"HIDDEN_UNITS\": 2048,\n",
    "    \"DROPOUT\": 0.2,\n",
    "    \n",
    "    # THAM SỐ CHO PHẦN SUBMIT\n",
    "    \"TOP_K_PER_PROTEIN\": 150,\n",
    "    \"GLOBAL_THRESHOLD_SEARCH\": True,\n",
    "    \"THRESHOLD_GRID\": [i/100 for i in range(1, 51)],\n",
    "    \n",
    "    # LAN TRUYỀN\n",
    "    \"PROPAGATE_TRAIN_LABELS\": True,\n",
    "    \"PROPAGATE_PREDICTIONS\": True,\n",
    "}\n",
    "\n",
    "print(\"Config loaded...\")\n",
    "random.seed(CONFIG[\"RANDOM_SEED\"])\n",
    "np.random.seed(CONFIG[\"RANDOM_SEED\"])\n",
    "tf.random.set_seed(CONFIG[\"RANDOM_SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. Utils for TSV/OBO parsing \n",
    "# ------------------------------------------------------------\n",
    "def read_train_terms(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"protein\", \"go\", \"ont\"], dtype=str)\n",
    "    mapping = df.groupby(\"protein\")[\"go\"].apply(list).to_dict()\n",
    "    print(f\"[io] Read training annotations for {len(mapping)} proteins from {path}\")\n",
    "    return mapping\n",
    "\n",
    "def parse_obo(go_obo_path: str) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]:\n",
    "    parents = defaultdict(set); children = defaultdict(set)\n",
    "    if not os.path.exists(go_obo_path): return parents, children\n",
    "    with open(go_obo_path,\"r\") as f:\n",
    "        cur_id=None\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line==\"[Term]\": cur_id=None\n",
    "            elif line.startswith(\"id: \"): cur_id=line.split(\"id: \")[1].strip()\n",
    "            elif line.startswith(\"is_a: \"):\n",
    "                pid=line.split()[1].strip()\n",
    "                if cur_id: parents[cur_id].add(pid); children[pid].add(cur_id)\n",
    "            elif line.startswith(\"relationship: part_of \"):\n",
    "                parts=line.split(); \n",
    "                if len(parts)>=3:\n",
    "                    pid=parts[2].strip()\n",
    "                    if cur_id: parents[cur_id].add(pid); children[pid].add(cur_id)\n",
    "    print(f\"[io] Parsed OBO: {len(parents)} nodes with parents\")\n",
    "    return parents, children\n",
    "\n",
    "def get_ancestors(go_id: str, parents: Dict[str, Set[str]]) -> Set[str]:\n",
    "    ans=set(); stack=[go_id]\n",
    "    while stack:\n",
    "        cur=stack.pop()\n",
    "        for p in parents.get(cur,[]): \n",
    "            if p not in ans:\n",
    "                ans.add(p); stack.append(p)\n",
    "    return ans\n",
    "\n",
    "def read_IA(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"go\", \"score\"])\n",
    "    df[\"score\"] = df[\"score\"].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5. Load Metadata (Terms, OBO, IA)\n",
    "# ------------------------------------------------------------\n",
    "train_terms = read_train_terms(CONFIG[\"TRAIN_TERMS\"])\n",
    "parents_map, children_map = parse_obo(CONFIG[\"GO_OBO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. Load PRE-COMPUTED TRAIN EMBEDDINGS\n",
    "# ------------------------------------------------------------\n",
    "print(\"[io] Loading pre-computed TRAIN embeddings...\")\n",
    "# LOAD EMBEDDING \n",
    "train_emb_full = np.load(CONFIG[\"TRAIN_EMBEDS\"])\n",
    "train_ids_full = np.load(CONFIG[\"TRAIN_IDS\"])\n",
    "print(\"ID trước khi sửa:\", train_ids_full[0])\n",
    "train_ids_full = pd.Series(train_ids_full).apply(lambda x: x.split('|')[1]).values\n",
    "print(\"ID sau khi sửa:\", train_ids_full[0])\n",
    "print(f\"[io] Loaded train embeddings shape: {train_emb_full.shape}\")\n",
    "print(f\"[io] Loaded train ids shape: {train_ids_full.shape}\")\n",
    "\n",
    "# GIỮ LẠI CÁC PROTEIN GIAO NHAU (ĐÔI KHI EMBEDDING CÓ SỐ DÒNG LỚN HƠN)\n",
    "valid_indices = []\n",
    "valid_ids = []\n",
    "for idx, pid in enumerate(train_ids_full):\n",
    "    if pid in train_terms:\n",
    "        valid_indices.append(idx)\n",
    "        valid_ids.append(pid)\n",
    "\n",
    "X_emb = train_emb_full[valid_indices]\n",
    "X_proteins = valid_ids\n",
    "print(f\"[prep] Filtered to {len(X_proteins)} proteins that have labels.\")\n",
    "\n",
    "del train_emb_full, train_ids_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7. Label Processing (IA Scores & Propagation)\n",
    "# ------------------------------------------------------------\n",
    "# LAN TRUYỀN NHÃN\n",
    "if CONFIG[\"PROPAGATE_TRAIN_LABELS\"] and parents_map:\n",
    "    print(\"[prep] Propagating train labels up GO graph\")\n",
    "    propagated={}\n",
    "    for p in X_proteins:\n",
    "        terms=set(train_terms[p])\n",
    "        extra=set()\n",
    "        for t in list(terms): extra |= get_ancestors(t, parents_map)\n",
    "        propagated[p]=sorted(terms|extra)\n",
    "    for p in X_proteins:\n",
    "        train_terms[p] = propagated[p]\n",
    "\n",
    "IA_list = read_IA(CONFIG[\"IA_FILE\"])\n",
    "term_freq = Counter()\n",
    "for p in X_proteins:\n",
    "    term_freq.update(train_terms[p])\n",
    "\n",
    "IA_dict = dict(zip(IA_list[\"go\"], IA_list[\"score\"]))\n",
    "term_combined_score = {}\n",
    "for t in term_freq:\n",
    "    ia_score = IA_dict.get(t, 0)\n",
    "    term_combined_score[t] = ia_score * term_freq[t]\n",
    "\n",
    "if CONFIG[\"TOP_K_LABELS\"] is not None:\n",
    "    top_terms_sorted = sorted(term_combined_score.items(), key=lambda x: x[1], reverse=True)\n",
    "    chosen_terms = set([t for t, _ in top_terms_sorted[:CONFIG[\"TOP_K_LABELS\"]]])\n",
    "    print(f\"[prep] Restricting to top-{CONFIG['TOP_K_LABELS']} GO terms\")\n",
    "else:\n",
    "    chosen_terms = set(term_combined_score.keys())\n",
    "\n",
    "# NHÃN Y\n",
    "y_labels = [[t for t in train_terms[p] if t in chosen_terms] for p in X_proteins]\n",
    "mlb = MultiLabelBinarizer(classes=sorted(chosen_terms))\n",
    "Y = mlb.fit_transform(y_labels).astype(np.float32)\n",
    "print(\"[prep] Label matrix shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 8. Train / validation split\n",
    "# ------------------------------------------------------------\n",
    "y = Y\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_emb, y, test_size=0.1, random_state=CONFIG[\"RANDOM_SEED\"]\n",
    ")\n",
    "print(\"[train] shapes:\", X_tr.shape, X_val.shape, y_tr.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 9. ĐỊNH NGHĨA HÀM FOCAL LOSS\n",
    "# ------------------------------------------------------------\n",
    "def focal_loss_fixed(gamma, alpha):\n",
    "    def focal_loss_fn(y_true, y_pred):\n",
    "        # KIỂU DỮ LIỆU FLOAT 32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        # Tính Focal Loss để giảm ảnh hưởng nhãn âm\n",
    "        # Công thức: -alpha * (1 - p)^gamma * log(p) \n",
    "        loss = alpha * tf.pow(1 - y_pred, gamma) * y_true * tf.math.log(y_pred) \\\n",
    "             + (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "             \n",
    "        # Trả về giá trị dương (trị tuyệt đối)\n",
    "        return -tf.reduce_mean(loss, axis=-1)\n",
    "        \n",
    "    return focal_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 10. ResNet MLP\n",
    "# ------------------------------------------------------------\n",
    "def build_resnet_model(input_dim, output_dim, hidden_units, dropout, lr, n_blocks):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # LỚP CHIẾU ( ĐƯA SỐ CHIỀU EMBEDDING LÀ 1280 LÊN SỐ CHIỀU TRONG CONFIG)\n",
    "    x = layers.Dense(hidden_units, activation=\"gelu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # RESIDUAL BLOCK\n",
    "    for i in range(n_blocks):\n",
    "        shortcut = x\n",
    "        \n",
    "        # Layer 1\n",
    "        res = layers.Dense(hidden_units, activation=\"gelu\")(x)\n",
    "        res = layers.BatchNormalization()(res)\n",
    "        res = layers.Dropout(dropout)(res)\n",
    "        \n",
    "        # Layer 2: Activation linear để cộng với shortcut\n",
    "        res = layers.Dense(hidden_units, activation=\"gelu\")(res)\n",
    "        res = layers.BatchNormalization()(res)\n",
    "        \n",
    "        # Add (ResNet Connection)\n",
    "        x = layers.Add()([x, res])\n",
    "        \n",
    "        # Activation sau khi cộng\n",
    "        x = layers.Activation('gelu')(x) # 0.273 voi relu\n",
    "\n",
    "    # Output Layer\n",
    "    # Multi-label classification -> dùng Sigmoid\n",
    "    outputs = layers.Dense(output_dim, activation=\"sigmoid\")(x) \n",
    "\n",
    "    # Tạo Model Object\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"ResNet_MLP\")\n",
    "\n",
    "    # Định nghĩa Schedule \n",
    "    steps_per_epoch = len(X_tr) // CONFIG[\"BATCH_SIZE\"]\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=1e-3,             # Bắt đầu\n",
    "        decay_steps=CONFIG[\"EPOCHS\"] * steps_per_epoch, # Giảm dần\n",
    "        alpha=0.01                              # Kết thúc ở 1% LR ban đầu\n",
    "    )\n",
    "\n",
    "    # Khởi tạo Optimizer\n",
    "    try:\n",
    "        \n",
    "        opt = optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-2)\n",
    "    except AttributeError:\n",
    "        # NẾU LỖI DÙNG ADAM\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # Compile \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=focal_loss_fixed(gamma=2.0, alpha=0.35),\n",
    "        metrics=[metrics.BinaryAccuracy(name='acc'), metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 11. Khởi tạo và Run\n",
    "# ------------------------------------------------------------\n",
    "# Giả lập chiều dữ liệu\n",
    "D = X_tr.shape[1] \n",
    "M = y_tr.shape[1]\n",
    "\n",
    "model = build_resnet_model(\n",
    "    input_dim=D, \n",
    "    output_dim=CONFIG[\"TOP_K_LABELS\"],\n",
    "    hidden_units=CONFIG[\"HIDDEN_UNITS\"],\n",
    "    dropout=CONFIG[\"DROPOUT\"],\n",
    "    lr=CONFIG[\"LEARNING_RATE\"],\n",
    "    n_blocks=2\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# Checkpoint & Early Stopping\n",
    "ckpt_path = \"/kaggle/working/best_resnet_model.keras\" \n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1)\n",
    "mc = callbacks.ModelCheckpoint(\"/kaggle/working/best_resnet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 12. Train \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "rlr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    factor=0.5,   \n",
    "    patience=3,     \n",
    "    verbose=1, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_tr, y_tr, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=CONFIG[\"EPOCHS\"], \n",
    "    batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "    callbacks=[es, mc, rlr],\n",
    "    verbose=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 13. Evaluate & select global threshold\n",
    "# ------------------------------------------------------------\n",
    "def read_IA_safe(path):\n",
    "    if not os.path.exists(path): return {}\n",
    "    df=pd.read_csv(path, sep=\"\\t\", header=None, names=[\"go\",\"ia\"], dtype=str)\n",
    "    d={}\n",
    "    for _,r in df.iterrows():\n",
    "        try: d[r.go]=float(r.ia)\n",
    "        except: \n",
    "            try: d[r.go]=float(r.ia.replace(\",\",\".\")) \n",
    "            except: d[r.go]=0.0\n",
    "    return d\n",
    "\n",
    "ia_weights = read_IA_safe(CONFIG[\"IA_FILE\"])\n",
    "\n",
    "def weighted_precision_recall_f1(y_true, y_pred_bin, mlb_obj):\n",
    "    tp = ((y_true==1)&(y_pred_bin==1)).sum(axis=0).astype(float)\n",
    "    fp = ((y_true==0)&(y_pred_bin==1)).sum(axis=0).astype(float)\n",
    "    fn = ((y_true==1)&(y_pred_bin==0)).sum(axis=0).astype(float)\n",
    "    eps=1e-12\n",
    "    prec = tp/(tp+fp+eps); rec = tp/(tp+fn+eps)\n",
    "    f1 = 2*prec*rec/(prec+rec+eps)\n",
    "    cls = mlb_obj.classes_\n",
    "    weights = np.array([ia_weights.get(c,1.0) for c in cls], dtype=float)\n",
    "    weighted_f1 = (f1*weights).sum()/(weights.sum()+eps)\n",
    "    return weighted_f1\n",
    "\n",
    "y_val_prob = model.predict(X_val, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\n",
    "best_thresh = 0.5; best_score = -1.0\n",
    "if CONFIG[\"GLOBAL_THRESHOLD_SEARCH\"]:\n",
    "    for t in CONFIG[\"THRESHOLD_GRID\"]:\n",
    "        y_pred_bin = (y_val_prob >= t).astype(int)\n",
    "        f1 = weighted_precision_recall_f1(y_val, y_pred_bin, mlb)\n",
    "        if f1 > best_score: best_score=f1; best_thresh=t\n",
    "print(f\"[eval] best_thresh {best_thresh} best IA-weighted F1 {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 14. Inference on TEST set using Pre-computed Embeddings\n",
    "# ------------------------------------------------------------\n",
    "print(\"[test] Loading pre-computed TEST embeddings...\")\n",
    "test_emb_full = np.load(CONFIG[\"TEST_EMBEDS\"]) \n",
    "test_ids_full = np.load(CONFIG[\"TEST_IDS\"])\n",
    "N_test = len(test_ids_full)\n",
    "print(f\"[test] Loaded {N_test} test embeddings. Shape: {test_emb_full.shape}\")\n",
    "\n",
    "term_to_idx = {t:i for i,t in enumerate(mlb.classes_)}\n",
    "restricted_parents = {}\n",
    "for t in mlb.classes_:\n",
    "    restricted_parents[t] = set([p for p in parents_map.get(t, set()) if p in term_to_idx])\n",
    "\n",
    "def propagate_batch(pred_batch: np.ndarray, parents_map_local: Dict[str, Set[str]], classes_list: List[str], iterations=3):\n",
    "    B, Mloc = pred_batch.shape\n",
    "    idx_map = {i:classes_list[i] for i in range(Mloc)}\n",
    "    term_to_idx_local = {classes_list[i]: i for i in range(Mloc)}\n",
    "    for _ in range(iterations):\n",
    "        changed = False\n",
    "        for child_idx in range(Mloc):\n",
    "            child_term = idx_map[child_idx]\n",
    "            child_scores = pred_batch[:, child_idx]\n",
    "            for pterm in parents_map_local.get(child_term, []):\n",
    "                pidx = term_to_idx_local[pterm]\n",
    "                mask = child_scores > pred_batch[:, pidx]\n",
    "                if mask.any():\n",
    "                    pred_batch[mask, pidx] = child_scores[mask]\n",
    "                    changed = True\n",
    "        if not changed: break\n",
    "    return pred_batch\n",
    "\n",
    "# Open submission file\n",
    "out_fpath = CONFIG[\"OUTPUT_SUBMISSION\"]\n",
    "open(out_fpath, \"w\").close() \n",
    "out_f = open(out_fpath, \"a\")\n",
    "\n",
    "print(f\"[test] Predicting in batches of {CONFIG['PREDICT_BATCH_SIZE']}...\")\n",
    "\n",
    "for i in range(0, N_test, CONFIG[\"PREDICT_BATCH_SIZE\"]):\n",
    "    X_batch = test_emb_full[i : i + CONFIG[\"PREDICT_BATCH_SIZE\"]]\n",
    "    ids_batch = test_ids_full[i : i + CONFIG[\"PREDICT_BATCH_SIZE\"]]\n",
    "    \n",
    "    # đoán\n",
    "    y_batch_prob = model.predict(X_batch, batch_size=min(128, len(X_batch)), verbose=0)\n",
    "    \n",
    "    # lan truyền\n",
    "    if CONFIG[\"PROPAGATE_PREDICTIONS\"] and parents_map:\n",
    "        y_batch_prob = propagate_batch(y_batch_prob, restricted_parents, list(mlb.classes_), iterations=3)\n",
    "        \n",
    "    # Write submission\n",
    "    for ridx, pid in enumerate(ids_batch):\n",
    "        probs = y_batch_prob[ridx]\n",
    "        \n",
    "        # Filter top-k\n",
    "        top_k = CONFIG[\"TOP_K_PER_PROTEIN\"]\n",
    "        idxs = np.argsort(probs)[-top_k:]\n",
    "        idxs = [int(x) for x in idxs if probs[x] > 1e-3]\n",
    "        idxs = sorted(idxs, key=lambda x: probs[x], reverse=True)\n",
    "        \n",
    "        for idx in idxs:\n",
    "            score = float(probs[idx])\n",
    "            go_id = mlb.classes_[idx]\n",
    "            out_f.write(f\"{pid}\\t{go_id}\\t{score:.3f}\\n\")\n",
    "            \n",
    "    if (i // CONFIG[\"PREDICT_BATCH_SIZE\"]) % 10 == 0:\n",
    "        out_f.flush()\n",
    "        print(f\"[stream] processed {min(i + CONFIG['PREDICT_BATCH_SIZE'], N_test)} / {N_test}\")\n",
    "\n",
    "out_f.close()\n",
    "print(f\"[done] Submission written to {CONFIG['OUTPUT_SUBMISSION']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 9014025,
     "sourceId": 14144139,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
