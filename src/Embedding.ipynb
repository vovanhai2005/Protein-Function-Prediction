{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers biopython torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "TRAIN_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\n",
    "TEST_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"\n",
    "\n",
    "# Cáº¥u hÃ¬nh pháº§n cá»©ng Ä‘á»ƒ train:\n",
    "BATCH_SIZE = 6 \n",
    "MAX_CONTEXT = 1024 \n",
    "EMBED_DIM = 1280  # Model 650M táº¡o ra vector 1280 chiá»u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# khá»Ÿi táº¡o\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Device: {device}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.half()\n",
    "    \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Xá»­ lÃ½ chuá»—i dÃ i hÆ¡n 1024 kÃ½ tá»±\n",
    "def generate_embeddings_650M(fasta_path, prefix_name):\n",
    "    print(f\"\\n--- Processing {prefix_name.upper()} set with ESM2-650M ---\")\n",
    "    \n",
    "    if not os.path.exists(fasta_path):\n",
    "        print(f\"âš ï¸ Warning: File {fasta_path} not found.\")\n",
    "        return\n",
    "\n",
    "    # Load dá»¯ liá»‡u\n",
    "    ids = []\n",
    "    seqs = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        ids.append(record.id)\n",
    "        seqs.append(str(record.seq))\n",
    "    \n",
    "    print(f\"Found {len(seqs)} sequences.\")\n",
    "    \n",
    "    # Sáº¯p xáº¿p theo Ä‘á»™ dÃ i Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™\n",
    "    sorted_indices = np.argsort([len(s) for s in seqs])\n",
    "    seqs_sorted = [seqs[i] for i in sorted_indices]\n",
    "    \n",
    "    batch_embeddings_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(seqs_sorted), BATCH_SIZE), desc=f\"Embedding {prefix_name}\"):\n",
    "        batch_seqs = seqs_sorted[i : i + BATCH_SIZE]\n",
    "        \n",
    "        has_long_seq = any(len(s) > (MAX_CONTEXT - 2) for s in batch_seqs)\n",
    "        \n",
    "        if has_long_seq:\n",
    "            # Chunking cho protein dÃ i hÆ¡n 1022 kÃ½ tá»±\n",
    "            for seq in batch_seqs:\n",
    "                chunks = [seq[j : j + (MAX_CONTEXT-2)] for j in range(0, len(seq), (MAX_CONTEXT-2))]\n",
    "                chunk_vectors = []\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    inputs = tokenizer(chunk, return_tensors=\"pt\", padding=False, truncation=True, max_length=MAX_CONTEXT).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        out = model(**inputs).last_hidden_state\n",
    "                        if out.shape[1] > 2: vec = out[0, 1:-1, :].mean(dim=0)\n",
    "                        else: vec = out[0, :, :].mean(dim=0)\n",
    "                        chunk_vectors.append(vec.float().cpu().numpy())\n",
    "                \n",
    "                # TÃ­nh trung bÃ¬nh cÃ¡c Ä‘oáº¡n\n",
    "                batch_embeddings_list.append(np.mean(chunk_vectors, axis=0))\n",
    "                \n",
    "        else:\n",
    "            # --- CHIáº¾N LÆ¯á»¢C BATCH NHANH ---\n",
    "            inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_CONTEXT).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                last_hidden_state = outputs.last_hidden_state\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                \n",
    "                # Masking chuáº©n\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                if device.type == 'cuda': mask_expanded = mask_expanded.half()\n",
    "\n",
    "                sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "                sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "                \n",
    "                # Káº¿t quáº£ batch\n",
    "                batch_embs = (sum_embeddings / sum_mask).float().cpu().numpy()\n",
    "                batch_embeddings_list.extend(batch_embs)\n",
    "        \n",
    "        # Clear VRAM \n",
    "        if i % (BATCH_SIZE * 20) == 0: \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Sáº¯p xáº¿p láº¡i Ä‘Ãºng thá»© tá»± ban Ä‘áº§u\n",
    "    final_embeddings = np.zeros((len(seqs), EMBED_DIM), dtype=np.float32)\n",
    "    for idx, original_idx in enumerate(sorted_indices):\n",
    "        final_embeddings[original_idx] = batch_embeddings_list[idx]\n",
    "    \n",
    "    # LÆ°u file\n",
    "    np.save(f\"{prefix_name}_embeddings_650M.npy\", final_embeddings)\n",
    "    np.save(f\"{prefix_name}_ids.npy\", np.array(ids))\n",
    "    print(f\"âœ… Saved: {prefix_name}_embeddings_650M.npy ({final_embeddings.shape})\")\n",
    "    \n",
    "    # Giáº£i phÃ³ng RAM triá»‡t Ä‘á»ƒ\n",
    "    del final_embeddings, batch_embeddings_list, seqs, ids\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cháº¡y\n",
    "generate_embeddings_650M(TRAIN_FASTA, \"train\")\n",
    "generate_embeddings_650M(TEST_FASTA, \"test\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ DONE 650M PIPELINE!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
