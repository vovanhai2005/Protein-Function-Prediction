{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers biopython torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom Bio import SeqIO\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport gc\n\n# --- C·∫§U H√åNH CHO MODEL 650M ---\nMODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\nTRAIN_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTEST_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"\n\n# V·ªõi Model 650M tr√™n GPU T4 (16GB VRAM):\n# Batch size = 4-6 l√† an to√†n nh·∫•t. N·∫øu crash th√¨ gi·∫£m xu·ªëng 2.\nBATCH_SIZE = 6 \nMAX_CONTEXT = 1024 \nEMBED_DIM = 1280  # Model 650M t·∫°o ra vector 1280 chi·ªÅu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. KH·ªûI T·∫†O ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ Device: {device}\")\n\nprint(f\"Loading {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# B·∫Øt bu·ªôc d√πng FP16 ƒë·ªÉ ch·∫°y ƒë∆∞·ª£c Batch Size h·ª£p l√Ω\nif device.type == 'cuda':\n    model = model.half()\n    \nmodel.to(device)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. H√ÄM T·∫†O EMBEDDING (H·ªó tr·ª£ chu·ªói d√†i) ---\ndef generate_embeddings_650M(fasta_path, prefix_name):\n    print(f\"\\n--- Processing {prefix_name.upper()} set with ESM2-650M ---\")\n    \n    if not os.path.exists(fasta_path):\n        print(f\"‚ö†Ô∏è Warning: File {fasta_path} not found.\")\n        return\n\n    # Load d·ªØ li·ªáu\n    ids = []\n    seqs = []\n    for record in SeqIO.parse(fasta_path, \"fasta\"):\n        ids.append(record.id)\n        seqs.append(str(record.seq))\n    \n    print(f\"Found {len(seqs)} sequences.\")\n    \n    # S·∫Øp x·∫øp theo ƒë·ªô d√†i ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô (gi·∫£m padding th·ª´a)\n    sorted_indices = np.argsort([len(s) for s in seqs])\n    seqs_sorted = [seqs[i] for i in sorted_indices]\n    \n    batch_embeddings_list = []\n    \n    # B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p\n    for i in tqdm(range(0, len(seqs_sorted), BATCH_SIZE), desc=f\"Embedding {prefix_name}\"):\n        batch_seqs = seqs_sorted[i : i + BATCH_SIZE]\n        \n        # Ki·ªÉm tra xem c√≥ chu·ªói n√†o d√†i qu√° 1022 k√Ω t·ª± kh√¥ng\n        has_long_seq = any(len(s) > (MAX_CONTEXT - 2) for s in batch_seqs)\n        \n        if has_long_seq:\n            # --- CHI·∫æN L∆Ø·ª¢C CHUNKING (C·∫Øt nh·ªè & G·ªôp) ---\n            # D√†nh cho protein d√†i ƒë·ªÉ kh√¥ng b·ªã m·∫•t th√¥ng tin\n            for seq in batch_seqs:\n                chunks = [seq[j : j + (MAX_CONTEXT-2)] for j in range(0, len(seq), (MAX_CONTEXT-2))]\n                chunk_vectors = []\n                \n                for chunk in chunks:\n                    inputs = tokenizer(chunk, return_tensors=\"pt\", padding=False, truncation=True, max_length=MAX_CONTEXT).to(device)\n                    with torch.no_grad():\n                        out = model(**inputs).last_hidden_state\n                        # Mean pooling (b·ªè CLS/EOS)\n                        if out.shape[1] > 2: vec = out[0, 1:-1, :].mean(dim=0)\n                        else: vec = out[0, :, :].mean(dim=0)\n                        chunk_vectors.append(vec.float().cpu().numpy())\n                \n                # T√≠nh trung b√¨nh c√°c ƒëo·∫°n\n                batch_embeddings_list.append(np.mean(chunk_vectors, axis=0))\n                \n        else:\n            # --- CHI·∫æN L∆Ø·ª¢C BATCH NHANH ---\n            inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_CONTEXT).to(device)\n            with torch.no_grad():\n                outputs = model(**inputs)\n                last_hidden_state = outputs.last_hidden_state\n                attention_mask = inputs['attention_mask']\n                \n                # Masking chu·∫©n\n                mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n                if device.type == 'cuda': mask_expanded = mask_expanded.half()\n\n                sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n                sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n                \n                # K·∫øt qu·∫£ batch\n                batch_embs = (sum_embeddings / sum_mask).float().cpu().numpy()\n                batch_embeddings_list.extend(batch_embs)\n        \n        # Clear VRAM th∆∞·ªùng xuy√™n h∆°n v√¨ model 650M kh√° n·∫∑ng\n        if i % (BATCH_SIZE * 20) == 0: \n            torch.cuda.empty_cache()\n\n    # S·∫Øp x·∫øp l·∫°i ƒë√∫ng th·ª© t·ª± ban ƒë·∫ßu\n    final_embeddings = np.zeros((len(seqs), EMBED_DIM), dtype=np.float32)\n    for idx, original_idx in enumerate(sorted_indices):\n        final_embeddings[original_idx] = batch_embeddings_list[idx]\n    \n    # L∆∞u file\n    # ƒê·∫∑t t√™n suffix _650M ƒë·ªÉ d·ªÖ ph√¢n bi·ªát\n    np.save(f\"{prefix_name}_embeddings_650M.npy\", final_embeddings)\n    np.save(f\"{prefix_name}_ids.npy\", np.array(ids))\n    print(f\"‚úÖ Saved: {prefix_name}_embeddings_650M.npy ({final_embeddings.shape})\")\n    \n    # Gi·∫£i ph√≥ng RAM tri·ªát ƒë·ªÉ\n    del final_embeddings, batch_embeddings_list, seqs, ids\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. CH·∫†Y ---\ngenerate_embeddings_650M(TRAIN_FASTA, \"train\")\ngenerate_embeddings_650M(TEST_FASTA, \"test\")\n\nprint(\"\\nüéâ DONE 650M PIPELINE!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}